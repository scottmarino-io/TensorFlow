{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scottmarino-io/TensorFlow/blob/main/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Constructing a Text Generation Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GbGfr_oLCat"
      },
      "source": [
        "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4Bf5FVHfganK",
        "outputId": "837598fb-3b66-4214-911b-3986410eef17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-01 16:33:23--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.199.113, 74.125.199.101, 74.125.199.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.199.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 [following]\n",
            "--2024-03-01 16:33:23--  https://drive.usercontent.google.com/download?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.195.132, 2607:f8b0:400e:c02::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [application/octet-stream]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv   100%[===================>]  69.08M  65.8MB/s    in 1.0s    \n",
            "\n",
            "2024-03-01 16:33:29 (65.8 MB/s) - ‘/tmp/songdata.csv’ saved [72436445/72436445]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu1BTzMIS1oy"
      },
      "source": [
        "## **First 10 Songs**\n",
        "\n",
        "Let's first look at just 10 songs from the dataset, and see how things perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmb9rGaAUDO-"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2AVAvyF_Vuh5"
      },
      "outputs": [],
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "apcEXp7WhVBs",
        "outputId": "ce8e1127-5d82-4e1a-b2cd-d0bfba75e72c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-fbdddccf8583>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n"
          ]
        }
      ],
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9x68iN_X6FK"
      },
      "source": [
        "### Create Sequences and Labels\n",
        "\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QmlTsUqfikVO"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length\n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zsmu3aEId49i",
        "outputId": "1dc332cd-817b-4919-8758-98149b256ad0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1TAJMlmfO8r"
      },
      "source": [
        "### Train a Text Generation Model\n",
        "\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "G1YXuxIqfygN",
        "outputId": "3604c787-cb5a-41e9-dfa7-b1376267c2e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 10s 80ms/step - loss: 6.0066 - accuracy: 0.0298\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 1s 17ms/step - loss: 5.4455 - accuracy: 0.0399\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 5.3725 - accuracy: 0.0399\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 1s 17ms/step - loss: 5.3191 - accuracy: 0.0383\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.2450 - accuracy: 0.0394\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.1789 - accuracy: 0.0394\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 5.1218 - accuracy: 0.0388\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 1s 15ms/step - loss: 5.0554 - accuracy: 0.0409\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.9985 - accuracy: 0.0409\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.9143 - accuracy: 0.0449\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.8350 - accuracy: 0.0520\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.7577 - accuracy: 0.0575\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.6866 - accuracy: 0.0580\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.6070 - accuracy: 0.0716\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.5405 - accuracy: 0.0742\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.4713 - accuracy: 0.0817\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.4086 - accuracy: 0.0853\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.3375 - accuracy: 0.1065\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.2760 - accuracy: 0.1080\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.2132 - accuracy: 0.1110\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.1567 - accuracy: 0.1231\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.1095 - accuracy: 0.1256\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.0477 - accuracy: 0.1327\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.9875 - accuracy: 0.1332\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.9460 - accuracy: 0.1423\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.8850 - accuracy: 0.1468\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.8261 - accuracy: 0.1609\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.7688 - accuracy: 0.1736\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 3.7134 - accuracy: 0.1958\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.6661 - accuracy: 0.2013\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 3.6131 - accuracy: 0.2094\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.5470 - accuracy: 0.2195\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.4942 - accuracy: 0.2321\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.4428 - accuracy: 0.2487\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.3965 - accuracy: 0.2593\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.3498 - accuracy: 0.2725\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.3030 - accuracy: 0.2785\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.2595 - accuracy: 0.2911\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.2249 - accuracy: 0.2947\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.1707 - accuracy: 0.3214\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.1181 - accuracy: 0.3330\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.0617 - accuracy: 0.3350\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.0189 - accuracy: 0.3572\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9806 - accuracy: 0.3597\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9639 - accuracy: 0.3623\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9148 - accuracy: 0.3804\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.8779 - accuracy: 0.3814\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.8359 - accuracy: 0.4031\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.7870 - accuracy: 0.4026\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.7336 - accuracy: 0.4263\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6995 - accuracy: 0.4314\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.6626 - accuracy: 0.4334\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6016 - accuracy: 0.4511\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 2.5698 - accuracy: 0.4637\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.5366 - accuracy: 0.4697\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.4924 - accuracy: 0.4738\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.4513 - accuracy: 0.4834\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.4079 - accuracy: 0.4929\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3689 - accuracy: 0.5040\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3466 - accuracy: 0.5050\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3015 - accuracy: 0.5141\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2812 - accuracy: 0.5177\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2551 - accuracy: 0.5227\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2170 - accuracy: 0.5363\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.1855 - accuracy: 0.5348\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1535 - accuracy: 0.5419\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1285 - accuracy: 0.5444\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0966 - accuracy: 0.5570\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0614 - accuracy: 0.5641\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0801 - accuracy: 0.5530\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0137 - accuracy: 0.5681\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9901 - accuracy: 0.5747\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9681 - accuracy: 0.5807\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9342 - accuracy: 0.5858\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8970 - accuracy: 0.5959\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8718 - accuracy: 0.6009\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8334 - accuracy: 0.6065\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8111 - accuracy: 0.6196\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7937 - accuracy: 0.6226\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.7616 - accuracy: 0.6271\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.7556 - accuracy: 0.6256\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.7218 - accuracy: 0.6271\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.6843 - accuracy: 0.6398\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.6573 - accuracy: 0.6478\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6446 - accuracy: 0.6519\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6241 - accuracy: 0.6458\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6413 - accuracy: 0.6458\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6150 - accuracy: 0.6539\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5693 - accuracy: 0.6625\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5302 - accuracy: 0.6710\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5030 - accuracy: 0.6842\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4857 - accuracy: 0.6821\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4732 - accuracy: 0.6796\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4576 - accuracy: 0.6852\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4405 - accuracy: 0.6872\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4081 - accuracy: 0.6973\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3965 - accuracy: 0.6973\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3731 - accuracy: 0.7013\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3641 - accuracy: 0.6993\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3479 - accuracy: 0.7038\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3445 - accuracy: 0.7094\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3307 - accuracy: 0.7003\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3218 - accuracy: 0.7114\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3257 - accuracy: 0.7043\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3151 - accuracy: 0.7059\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2858 - accuracy: 0.7134\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.2547 - accuracy: 0.7200\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.2312 - accuracy: 0.7321\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.2349 - accuracy: 0.7301\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.2169 - accuracy: 0.7275\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1986 - accuracy: 0.7336\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1940 - accuracy: 0.7311\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1879 - accuracy: 0.7351\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1682 - accuracy: 0.7417\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1489 - accuracy: 0.7447\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1326 - accuracy: 0.7518\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1239 - accuracy: 0.7528\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1056 - accuracy: 0.7523\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0884 - accuracy: 0.7573\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0797 - accuracy: 0.7568\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0661 - accuracy: 0.7614\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0528 - accuracy: 0.7608\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0438 - accuracy: 0.7714\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0421 - accuracy: 0.7694\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 1.0295 - accuracy: 0.7634\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 1.0414 - accuracy: 0.7614\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.0156 - accuracy: 0.7725\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.9942 - accuracy: 0.7770\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.0034 - accuracy: 0.7719\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0260 - accuracy: 0.7684\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.9901 - accuracy: 0.7740\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.9788 - accuracy: 0.7775\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.9716 - accuracy: 0.7730\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.9652 - accuracy: 0.7800\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.9516 - accuracy: 0.7820\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9371 - accuracy: 0.7856\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9262 - accuracy: 0.7841\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9341 - accuracy: 0.7871\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9074 - accuracy: 0.7886\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8938 - accuracy: 0.7967\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8785 - accuracy: 0.7997\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8651 - accuracy: 0.8047\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.8544 - accuracy: 0.8027\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8514 - accuracy: 0.8063\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8403 - accuracy: 0.8108\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8373 - accuracy: 0.8093\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8394 - accuracy: 0.8042\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8447 - accuracy: 0.8027\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8227 - accuracy: 0.8078\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8057 - accuracy: 0.8153\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8006 - accuracy: 0.8128\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7889 - accuracy: 0.8179\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7837 - accuracy: 0.8158\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7732 - accuracy: 0.8169\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7632 - accuracy: 0.8204\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7587 - accuracy: 0.8194\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7526 - accuracy: 0.8214\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7456 - accuracy: 0.8300\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.7413 - accuracy: 0.8325\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.7370 - accuracy: 0.8274\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.7268 - accuracy: 0.8254\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7218 - accuracy: 0.8295\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7506 - accuracy: 0.8204\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7393 - accuracy: 0.8269\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7117 - accuracy: 0.8305\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7034 - accuracy: 0.8280\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7103 - accuracy: 0.8330\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7808 - accuracy: 0.8133\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7273 - accuracy: 0.8204\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7045 - accuracy: 0.8300\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6830 - accuracy: 0.8355\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6647 - accuracy: 0.8396\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6573 - accuracy: 0.8451\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6453 - accuracy: 0.8461\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6402 - accuracy: 0.8481\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6357 - accuracy: 0.8466\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6273 - accuracy: 0.8476\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6247 - accuracy: 0.8446\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6273 - accuracy: 0.8461\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6131 - accuracy: 0.8496\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6147 - accuracy: 0.8476\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6104 - accuracy: 0.8491\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6214 - accuracy: 0.8451\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.6091 - accuracy: 0.8486\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.6052 - accuracy: 0.8451\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.6058 - accuracy: 0.8471\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5978 - accuracy: 0.8522\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6014 - accuracy: 0.8486\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.5911 - accuracy: 0.8502\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5835 - accuracy: 0.8582\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5769 - accuracy: 0.8567\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5726 - accuracy: 0.8602\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5669 - accuracy: 0.8577\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5610 - accuracy: 0.8653\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5566 - accuracy: 0.8607\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5552 - accuracy: 0.8643\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5509 - accuracy: 0.8613\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5461 - accuracy: 0.8658\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5408 - accuracy: 0.8678\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5373 - accuracy: 0.8648\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVFpoREhV6Y"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aeSNfS7uhch0",
        "outputId": "5bdbffe2-d792-4708-a73f-2ab603c58072",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rAgRpxYhjpB"
      },
      "source": [
        "### Generate new lyrics!\n",
        "\n",
        "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DC7zfcgviDTp",
        "outputId": "34fa5db9-fb3b-49ba-8815-1f381e6e547d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 912ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "im feeling chills me sing and i cry on you did before would dont came came leave once as grieving wrong wrong wrong wrong feeling me grow youre intention lost brother scars morning showing pavement care care found broken a feet none of over you life is on you dreams you dreams you would wanted came once wrong eyes sad of ride none of us life over you would weave care on bang on i found once as grieving wrong wrong bedumbedumdum wrong bedumbedumdum truth life over you dreams you would wanted would weave intention intention intention hand ground eyes none of us\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "\n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l10c03_nlp_constructing_text_generation_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}